{
  "paragraphs": [
    {
      "title": "Create Tables with partition",
      "text": "spark.sql(\u0027use lens_targeting\u0027)\n\nsentiment_cleaned_sql \u003d \"\"\"\nCREATE TABLE if not exists sentiment_cleaned\n(gup_anon_id STRING, has_firefly_id double, tag STRING, freq double)\nPARTITIONED BY (partition_column STRING)\nSTORED AS ORC\n\"\"\"\n\nspark.sql(sentiment_cleaned_sql)\n\n\nsentiment_mapping_sql \u003d \"\"\"\nCREATE TABLE if not exists sentiment_mapping\n(gup_anon_id STRING, id double)\nPARTITIONED BY (partition_column STRING)\nSTORED AS ORC\n\"\"\"\n\nspark.sql(sentiment_mapping_sql)\n\n\n\nsentiment_predictions_sql \u003d \"\"\"\nCREATE TABLE if not exists sentiment_predictions\n(id double, prediction double)\nPARTITIONED BY (partition_column STRING)\nSTORED AS ORC\n\"\"\"\n\nspark.sql(sentiment_predictions_sql)\n",
      "user": "fberhane@gannett.com",
      "dateUpdated": "Oct 17, 2018 9:12:38 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "paragraphProgress": {
        "jobs": [],
        "numCompletedTasks": 0,
        "numTasks": 0,
        "truncated": false
      },
      "version": "v1",
      "jobName": "paragraph_1539650019317_746674853",
      "id": "20181016-003339_712054197_q_KS1P94R8591539268868",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "DataFrame[]\n"
      },
      "dateCreated": "Oct 16, 2018 12:33:39 AM",
      "dateSubmitted": "Oct 17, 2018 9:12:38 PM",
      "dateStarted": "Oct 17, 2018 9:12:39 PM",
      "dateFinished": "Oct 17, 2018 9:12:42 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 1000
    },
    {
      "text": "set_dynamic \u003d \"SET hive.exec.dynamic.partition \u003d true\"\nset_mode  \u003d \"SET hive.exec.dynamic.partition.mode \u003d nonstrict\"\nspark.sql(set_dynamic)\nspark.sql(set_mode)",
      "user": "fberhane@gannett.com",
      "dateUpdated": "Oct 17, 2018 7:51:11 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "version": "v1",
      "jobName": "paragraph_1539654651444_-1962142415",
      "id": "20181016-015051_1575422018_q_KS1P94R8591539268868",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "DataFrame[key: string, value: string]\n"
      },
      "dateCreated": "Oct 16, 2018 1:50:51 AM",
      "dateSubmitted": "Oct 17, 2018 7:51:11 PM",
      "dateStarted": "Oct 17, 2018 7:51:11 PM",
      "dateFinished": "Oct 17, 2018 7:51:11 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 1000
    },
    {
      "title": "Functions that clean sentiment data for provided date range, calculate PCA of TF-IDF and perform clustering based on the PCs that explain the  required variance",
      "text": "def get_min_pcs(variance_explained_list, variance_to_retain \u003d 99.0):\n  \"\"\"\n  It returns the number of PCs required to retain variance_to_retain (in percentage) of the variance\n  \"\"\"\n  \n  running_sum_list \u003d []\n  running_sum \u003d 0\n  for item in variance_explained_list:\n    running_sum +\u003d item\n    running_sum_list.append(running_sum)\n  bools \u003d [item \u003e\u003d variance_to_retain for item in running_sum_list]  \n  indices \u003d  [i for i, x in enumerate(bools) if x]\n  return min(indices)\n  \n  \n  \ndef identify_likely_bots(table, start_date, end_date, bot_threshold_percentage \u003d 0.0025):\n  \"\"\"\n  Returns the gup_anon_ids of the most active users for the date range provided\n  \"\"\"\n  \n  # Get how many users are likely bots\n  bots_sql \u003d \"\"\"select count(distinct gup_anon_id) * float(\u0027{bot_threshold_percentage}\u0027) * 0.01 from {table} \n            where load_date between \u0027{start_date}\u0027 and \u0027{end_date}\u0027 \n                  and (post_page_url !\u003d \u0027\u0027 or post_pagename !\u003d \u0027\u0027) \n                  and sub_section !\u003d \u0027\u0027\n                  and taxonomy_keywords !\u003d\u0027\u0027 \n                  and content_type  \u003d \u0027story pages\u0027\n            \"\"\".format(table \u003d table, start_date \u003d start_date, end_date \u003d end_date, bot_threshold_percentage \u003d bot_threshold_percentage)\n            \n  bots_count \u003d spark.sql(bots_sql).collect()[0][0]\n  \n  \n  # Sort users by page views in descending order and then add row number column\n  users_by_pageviews_sql \u003d \"\"\"select *, row_number() over (order by pageviews desc) as row_number from \n                     (select gup_anon_id, count(*) as pageviews from {table}  \n                      where load_date between \u0027{start_date}\u0027 and \u0027{end_date}\u0027 \n                            and (post_page_url !\u003d \u0027\u0027 or post_pagename !\u003d \u0027\u0027) \n                            and sub_section !\u003d \u0027\u0027 \n                            and taxonomy_keywords !\u003d\u0027\u0027\n                            and content_type  \u003d \u0027story pages\u0027 group by gup_anon_id)  order by pageviews desc\n                      \"\"\".format(table \u003d table, start_date \u003d start_date, end_date \u003d end_date)\n                      \n  users_by_pageviews \u003d spark.sql(users_by_pageviews_sql)\n  \n  \n  # Get the gup_anon_ids of the row numbers that are likely bots\n  bot_gup_anon_ids \u003d users_by_pageviews.filter(users_by_pageviews[\u0027row_number\u0027] \u003c\u003d bots_count).select(\u0027gup_anon_id\u0027)\n \n  return bot_gup_anon_ids\n  \n  \n  \ndef medium_high_cohorts_sentiment(table, start_date, end_date, pageviews_threshold \u003d 5, bot_threshold_percentage \u003d 0.0025):\n  \"\"\"\n  Returns the data for the medium and high cohorts for the provided date range after removing the likely bots,\n  and those that have pageviews less than pageviews_threshold\n  \"\"\"\n  \n  identify_likely_bots(table, start_date, end_date, bot_threshold_percentage).createOrReplaceTempView(\u0027likely_bots_table\u0027)\n  \n  users_sql \u003d \"\"\"select gup_anon_id, taxonomy_keywords, case when fire_fly_id \u003d \u0027undefined\u0027 or  fire_fly_id \u003d \u0027\u0027 \n              then 0 else 1 end as has_firefly_id from {table}  \n              where load_date between \u0027{start_date}\u0027 and \u0027{end_date}\u0027  \n                      and sub_section !\u003d \u0027\u0027  \n                      and (post_page_url !\u003d \u0027\u0027 or post_pagename !\u003d \u0027\u0027) \n                      and taxonomy_keywords !\u003d \u0027\u0027 and content_type  \u003d \u0027story pages\u0027\n                      \"\"\".format(table \u003d table, start_date \u003d start_date, end_date \u003d end_date)\n                      \n  spark.sql(users_sql).createOrReplaceTempView(\u0027users_table\u0027)\n  \n  \n  # Remove bots\n  spark.sql(\"\"\"select t1.* from users_table as t1 LEFT OUTER JOIN \n                    likely_bots_table as t2 on t1.gup_anon_id \u003d t2.gup_anon_id where t2.gup_anon_id is NULL\"\"\").createOrReplaceTempView(\u0027users_table_bots_removed\u0027)\n  \n  \n  # The gup_anon_ids of medium and high cohorts \n  medium_high_cohorts_sql \u003d \"select gup_anon_id, count(*) as count from users_table group by gup_anon_id having count(gup_anon_id) \u003e\u003d int(\u0027{pageviews_threshold}\u0027) \"\\\n  .format(pageviews_threshold \u003d pageviews_threshold)\n  \n  spark.sql(medium_high_cohorts_sql).createOrReplaceTempView(\u0027medium_high_cohorts_ids_table\u0027) \n  \n  \n  # Remove values less than pageviews_threshold\n  spark.sql(\"select t1.* from users_table_bots_removed as t1 inner join medium_high_cohorts_ids_table as t2 on t1.gup_anon_id \u003d t2.gup_anon_id\").createOrReplaceTempView(\u0027medium_high_cohorts_table\u0027)\n  \n  # If a given gup_anon_id has sometimes fire_fly_id and sometime not, change it to 1 and also explode taxonomy_keywords\n  return spark.sql(\"\"\"select gup_anon_id, tag, max(has_firefly_id) over (partition by gup_anon_id) as has_firefly_id from medium_high_cohorts_table\n                  lateral view explode(split(lower(taxonomy_keywords),\u0027,\u0027)) q1 as tag\"\"\")\n                  \n                  \n                  \ndef clean_sentiment(table, start_date, end_date, partition_column, pageviews_threshold \u003d 5, bot_threshold_percentage \u003d 0.0025):\n\n  \"\"\"\n  Returns cleaned and aggregated dataframe and writes a table of the cleaned data. \n  Note: start_date and end_date are used for table name\n  \"\"\"\n  \n  import pyspark.sql.functions as f\n\n  medium_high_cohorts_sentiment(table, start_date, end_date, pageviews_threshold, bot_threshold_percentage)\\\n  .withColumn(\u0027partition_column\u0027, f.lit(partition_column)).createOrReplaceTempView(\u0027medium_high_cohorts_table\u0027)\n  \n  # Aggregate  \n  \n  aggregate_sql \u003d \"\"\"select gup_anon_id, has_firefly_id, tag,  partition_column, count(*) as freq \n                  from medium_high_cohorts_table group by gup_anon_id, has_firefly_id, tag, partition_column\"\"\"\n               \n  spark.sql(aggregate_sql).createOrReplaceTempView(\u0027medium_high_cohorts_table_aggregated\u0027)\n      \n      \n  # Retain only sentiment tags\n  tags \u003d spark.sql(\"select * from zz_fish.sentiment_tags\").collect()\n  tags \u003d [tag[0] for tag in tags]\n  spark.sql(\"select * from medium_high_cohorts_table_aggregated\").filter(f.col(\u0027tag\u0027).isin(tags)).createOrReplaceTempView(\u0027medium_high_cohorts_sentiments_table\u0027)\n  \n  \n  # Write Table             \n  spark.sql(\"use lens_targeting\")\n  \n  insert_into_table_sql \u003d \"\"\"insert into sentiment_cleaned partition (partition_column) \n  select gup_anon_id, has_firefly_id, tag, freq, partition_column from medium_high_cohorts_sentiments_table\"\"\"\n  \n  spark.sql(insert_into_table_sql)\n  \n  return spark.sql(\"select * from medium_high_cohorts_sentiments_table\") \n  \n  \n  \ndef scale_sentiment(table, start_date, end_date, partition_column, pageviews_threshold \u003d 5, bot_threshold_percentage \u003d 0.0025 ):\n  \"\"\"\n  Returns the scaled column vector data for the selected date range \n  Note:\n     First the dataframe is pivoted into a dense matrix \n  \"\"\"\n  \n  from pyspark.ml.feature import StandardScaler\n  from pyspark.ml.feature import VectorAssembler\n  import numpy as np\n  from pyspark.ml.feature import IDF\n  \n  medium_high_cohorts_cleaned \u003d clean_sentiment(table, start_date, end_date,partition_column, pageviews_threshold, bot_threshold_percentage)  \n  \n  number_unique_sentiments \u003d medium_high_cohorts_cleaned.select(\u0027tag\u0027).distinct().count()\n  \n  # Pivot dataframe into a dense matrix\n  medium_high_cohorts_cleaned.groupBy([\"gup_anon_id\"]).pivot(\"tag\").sum(\"freq\").createOrReplaceTempView(\u0027medium_high_cohorts_pivoted_table\u0027)\n  \n  \n  # Add unique row numbers to be used as ID since the gup_anon_id is too long it cannot be used as ID since it gets truncated\n  \n  add_ids_sql \u003d \"\"\"\n        select *, row_number() over (order by gup_anon_id) as id, \u0027{partition_column}\u0027 as partition_column from medium_high_cohorts_pivoted_table\n  \"\"\".format(partition_column \u003d partition_column)\n  \n  spark.sql(add_ids_sql).createOrReplaceTempView(\u0027medium_high_cohorts_pivoted_table_with_ids\u0027) \n\n  \n  # Write ID-gup_anon_id mapping\n  spark.sql(\"use lens_targeting\")\n\n  insert_into_table_sql \u003d \"\"\"insert into sentiment_mapping partition (partition_column) \n                             select gup_anon_id, id, partition_column from medium_high_cohorts_pivoted_table_with_ids\"\"\"\n  \n  spark.sql(insert_into_table_sql)\n  \n  \n  \n  # Change to vector column\n  vector_column_input \u003d spark.sql(\"select * from medium_high_cohorts_pivoted_table_with_ids\").na.fill(0)\n  cols \u003d spark.sql(\"select * from medium_high_cohorts_pivoted_table_with_ids\").drop(\u0027gup_anon_id\u0027,\u0027id\u0027,\u0027partition_column\u0027).columns\n  assembler \u003d VectorAssembler(inputCols \u003d cols, outputCol \u003d \u0027features_assembler\u0027)\n  vector_column_output \u003d assembler.transform(vector_column_input)\n  \n  \n  # Apply Tf-IDF\n  idf \u003d IDF(inputCol \u003d assembler.getOutputCol(), outputCol\u003d\"features_idf\")\n  idf_model \u003d idf.fit(vector_column_output)\n  idf_rescaled_data \u003d idf_model.transform(vector_column_output)\n\n\n  # center the data\n  scaler \u003d StandardScaler(inputCol \u003d idf.getOutputCol(), outputCol \u003d \"scaledFeatures\", withStd\u003dFalse, withMean\u003dTrue)  # centered but not standardized\n  scaler_model \u003d scaler.fit(idf_rescaled_data)\n  scaled_data \u003d scaler_model.transform(idf_rescaled_data)\n  scaled_data \u003d scaled_data.select([\u0027id\u0027,\u0027scaledFeatures\u0027])\n  \n  return {\u0027scaled_data\u0027:scaled_data,\u0027number_unique_sentiments\u0027:number_unique_sentiments, \u0027columns\u0027:cols}\n  \n  \n  \ndef calculate_sentiment_pcs(table, start_date, end_date, partition_column, pageviews_threshold \u003d 5, bot_threshold_percentage \u003d 0.0025, variance_to_retain \u003d 99.0):\n  \"\"\"\n  Returns the PCs for the data of the selected date range\n \n  \"\"\"\n\n  from pyspark.ml.feature import PCA\n  from pyspark.ml.feature import VectorSlicer\n  \n  vector_column_output \u003d scale_sentiment(table, start_date, end_date, partition_column, pageviews_threshold, bot_threshold_percentage)\n  vector_column_dataframe \u003d vector_column_output[\u0027scaled_data\u0027]\n  number_unique_sentiments \u003d vector_column_output[\u0027number_unique_sentiments\u0027]\n  \n  pca \u003d PCA(k \u003d number_unique_sentiments, inputCol \u003d \"scaledFeatures\", outputCol\u003d\"pcaFeatures\")\n  model \u003d pca.fit(vector_column_dataframe)\n  transformed_feature \u003d model.transform(vector_column_dataframe).drop(\"scaledFeatures\")\n  \n  variance_explained_list \u003d [x * 100.00 for x in model.explainedVariance ]      # changing it to percentage\n  k_required \u003d get_min_pcs(variance_explained_list, variance_to_retain)\n  \n  slicer \u003d VectorSlicer(inputCol\u003d pca.getOutputCol(), outputCol\u003d\"features\", indices \u003d [x for x in range(k_required)])\n  \n  return slicer.transform(transformed_feature).drop(\"pcaFeatures\")\n  \n  \n  \ndef k_means_clustering_sentiment(table, start_date, end_date, partition_column, pageviews_threshold \u003d 5, bot_threshold_percentage \u003d 0.0025, variance_to_retain \u003d 99.0):\n  from pyspark.ml.clustering import KMeans\n  import pyspark.sql.functions as f\n  \n  PCs_dataframe \u003d calculate_sentiment_pcs(table, start_date, end_date, partition_column, pageviews_threshold, bot_threshold_percentage, variance_to_retain)\n  kmeans \u003d KMeans(maxIter\u003d50, featuresCol\u003d\"features\", predictionCol\u003d\"prediction\").setK(45).setSeed(1)\n  model \u003d kmeans.fit(PCs_dataframe)\n  transformed \u003d model.transform(PCs_dataframe)\n  transformed.select([\u0027id\u0027,\u0027prediction\u0027]).withColumn(\u0027partition_column\u0027, f.lit(partition_column)).createOrReplaceTempView(\u0027predictions_table\u0027)\n  \n  # Write predictions table\n  spark.sql(\"use lens_targeting\")\n\n  insert_into_table_sql \u003d \"\"\"insert into sentiment_predictions partition (partition_column) \n  select id, prediction, partition_column from predictions_table\"\"\"\n  \n  spark.sql(insert_into_table_sql)\n  \n  return transformed.select([\u0027id\u0027,\u0027prediction\u0027])",
      "user": "fberhane@gannett.com",
      "dateUpdated": "Oct 17, 2018 9:13:10 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "title": true,
        "lineNumbers": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "version": "v1",
      "jobName": "paragraph_1539268868880_-721579412",
      "id": "20181011-144108_744889650_q_KS1P94R8591539268868",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": ""
      },
      "dateCreated": "Oct 11, 2018 2:41:08 PM",
      "dateSubmitted": "Oct 17, 2018 9:13:10 PM",
      "dateStarted": "Oct 17, 2018 9:13:10 PM",
      "dateFinished": "Oct 17, 2018 9:13:10 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 1000
    },
    {
      "text": "import datetime\nnow \u003d datetime.datetime.now()\npartition_column \u003d str(now.year) + str(now.month) + str(now.day) + str(now.hour)\n\n\ndf \u003d k_means_clustering_sentiment(table \u003d \u0027omniture_l1.browser_view\u0027,\n                    start_date \u003d \"2018-09-01\",\n                   end_date \u003d \"2018-09-01\",\n                   partition_column \u003d partition_column,\n                   bot_threshold_percentage \u003d 0.0025).persist()",
      "user": "fberhane@gannett.com",
      "dateUpdated": "Oct 17, 2018 9:27:32 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "paragraphProgress": {
        "jobs": [
          {
            "id": 188,
            "jobUrl": "https://api.qubole.com/cluster-proxy?encodedUrl\u003dhttp%3A%2F%2F10.148.128.39%3A8088%2Fproxy%2Fapplication_1539793487822_0001/jobs/job?spark\u003dtrue\u0026id\u003d188",
            "numTasks": 2221,
            "numCompletedTasks": 2221,
            "stages": [
              {
                "id": 767,
                "completed": true,
                "stageUrl": "https://api.qubole.com/cluster-proxy?encodedUrl\u003dhttp%3A%2F%2F10.148.128.39%3A8088%2Fproxy%2Fapplication_1539793487822_0001/stages/stage/?id\u003d767\u0026attempt\u003d0",
                "numCompleteTasks": 1020,
                "numActiveTasks": 0,
                "numFailedTasks": 0,
                "numTotalTasks": 1020
              },
              {
                "id": 768,
                "completed": true,
                "stageUrl": "https://api.qubole.com/cluster-proxy?encodedUrl\u003dhttp%3A%2F%2F10.148.128.39%3A8088%2Fproxy%2Fapplication_1539793487822_0001/stages/stage/?id\u003d768\u0026attempt\u003d0",
                "numCompleteTasks": 1200,
                "numActiveTasks": 0,
                "numFailedTasks": 0,
                "numTotalTasks": 1200
              },
              {
                "id": 769,
                "completed": true,
                "stageUrl": "https://api.qubole.com/cluster-proxy?encodedUrl\u003dhttp%3A%2F%2F10.148.128.39%3A8088%2Fproxy%2Fapplication_1539793487822_0001/stages/stage/?id\u003d769\u0026attempt\u003d0",
                "numCompleteTasks": 1,
                "numActiveTasks": 0,
                "numFailedTasks": 0,
                "numTotalTasks": 1
              }
            ],
            "status": "Success"
          },
          {
            "id": 189,
            "jobUrl": "https://api.qubole.com/cluster-proxy?encodedUrl\u003dhttp%3A%2F%2F10.148.128.39%3A8088%2Fproxy%2Fapplication_1539793487822_0001/jobs/job?spark\u003dtrue\u0026id\u003d189",
            "numTasks": 500,
            "numCompletedTasks": 500,
            "stages": [
              {
                "id": 770,
                "completed": true,
                "stageUrl": "https://api.qubole.com/cluster-proxy?encodedUrl\u003dhttp%3A%2F%2F10.148.128.39%3A8088%2Fproxy%2Fapplication_1539793487822_0001/stages/stage/?id\u003d770\u0026attempt\u003d0",
                "numCompleteTasks": 500,
                "numActiveTasks": 0,
                "numFailedTasks": 0,
                "numTotalTasks": 500
              }
            ],
            "status": "Success"
          },
          {
            "id": 190,
            "jobUrl": "https://api.qubole.com/cluster-proxy?encodedUrl\u003dhttp%3A%2F%2F10.148.128.39%3A8088%2Fproxy%2Fapplication_1539793487822_0001/jobs/job?spark\u003dtrue\u0026id\u003d190",
            "numTasks": 4440,
            "numCompletedTasks": 1265,
            "stages": [
              {
                "id": 771,
                "completed": false,
                "stageUrl": "https://api.qubole.com/cluster-proxy?encodedUrl\u003dhttp%3A%2F%2F10.148.128.39%3A8088%2Fproxy%2Fapplication_1539793487822_0001/stages/stage/?id\u003d771\u0026attempt\u003d0",
                "numCompleteTasks": 9,
                "numActiveTasks": 0,
                "numFailedTasks": 0,
                "numTotalTasks": 10
              },
              {
                "id": 772,
                "completed": false,
                "stageUrl": "https://api.qubole.com/cluster-proxy?encodedUrl\u003dhttp%3A%2F%2F10.148.128.39%3A8088%2Fproxy%2Fapplication_1539793487822_0001/stages/stage/?id\u003d772\u0026attempt\u003d0",
                "numCompleteTasks": 8,
                "numActiveTasks": 0,
                "numFailedTasks": 0,
                "numTotalTasks": 44
              },
              {
                "id": 773,
                "completed": false,
                "stageUrl": "https://api.qubole.com/cluster-proxy?encodedUrl\u003dhttp%3A%2F%2F10.148.128.39%3A8088%2Fproxy%2Fapplication_1539793487822_0001/stages/stage/?id\u003d773\u0026attempt\u003d0",
                "numCompleteTasks": 1096,
                "numActiveTasks": 53,
                "numFailedTasks": 57,
                "numTotalTasks": 125
              },
              {
                "id": 774,
                "completed": false,
                "stageUrl": "https://api.qubole.com/cluster-proxy?encodedUrl\u003dhttp%3A%2F%2F10.148.128.39%3A8088%2Fproxy%2Fapplication_1539793487822_0001/stages/stage/?id\u003d774\u0026attempt\u003d0",
                "numCompleteTasks": 0,
                "numActiveTasks": 0,
                "numFailedTasks": 0,
                "numTotalTasks": 1200
              }
            ],
            "status": "Failed"
          }
        ],
        "numCompletedTasks": 3986,
        "numTasks": 7161,
        "truncated": false
      },
      "version": "v1",
      "jobName": "paragraph_1539721251794_-1724710846",
      "id": "20181016-202051_1969223073_q_KS1P94R8591539268868",
      "dateCreated": "Oct 16, 2018 8:20:51 PM",
      "dateSubmitted": "Oct 17, 2018 9:27:32 PM",
      "dateStarted": "Oct 17, 2018 9:27:32 PM",
      "dateFinished": "Oct 17, 2018 9:37:09 PM",
      "status": "ERROR",
      "errorMessage": "",
      "progressUpdateIntervalMs": 1000
    },
    {
      "text": "df.show()",
      "user": "fberhane@gannett.com",
      "dateUpdated": "Oct 17, 2018 9:18:48 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "JOB UI",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            "https://api.qubole.com/cluster-proxy?encodedUrl\u003dhttp%3A%2F%2F10.148.128.39%3A8088%2Fproxy%2Fapplication_1539793487822_0001/jobs/job?spark\u003dtrue\u0026id\u003d171"
          ],
          "interpreterSettingId": "2DM2QAA2B656181532106861417"
        }
      },
      "paragraphProgress": {
        "jobs": [
          {
            "id": 171,
            "jobUrl": "https://api.qubole.com/cluster-proxy?encodedUrl\u003dhttp%3A%2F%2F10.148.128.39%3A8088%2Fproxy%2Fapplication_1539793487822_0001/jobs/job?spark\u003dtrue\u0026id\u003d171",
            "numTasks": 6841,
            "numCompletedTasks": 4801,
            "stages": [
              {
                "id": 676,
                "completed": true,
                "stageUrl": "https://api.qubole.com/cluster-proxy?encodedUrl\u003dhttp%3A%2F%2F10.148.128.39%3A8088%2Fproxy%2Fapplication_1539793487822_0001/stages/stage/?id\u003d676\u0026attempt\u003d0",
                "numCompleteTasks": 1,
                "numActiveTasks": 0,
                "numFailedTasks": 0,
                "numTotalTasks": 1
              },
              {
                "id": 673,
                "completed": true,
                "stageUrl": "https://api.qubole.com/cluster-proxy?encodedUrl\u003dhttp%3A%2F%2F10.148.128.39%3A8088%2Fproxy%2Fapplication_1539793487822_0001/stages/stage/?id\u003d673\u0026attempt\u003d0",
                "numCompleteTasks": 1200,
                "numActiveTasks": 0,
                "numFailedTasks": 0,
                "numTotalTasks": 1200
              },
              {
                "id": 670,
                "completed": false,
                "stageUrl": "https://api.qubole.com/cluster-proxy?encodedUrl\u003dhttp%3A%2F%2F10.148.128.39%3A8088%2Fproxy%2Fapplication_1539793487822_0001/stages/stage/?id\u003d670\u0026attempt\u003d0",
                "numCompleteTasks": 0,
                "numActiveTasks": 0,
                "numFailedTasks": 0,
                "numTotalTasks": 1020
              },
              {
                "id": 674,
                "completed": true,
                "stageUrl": "https://api.qubole.com/cluster-proxy?encodedUrl\u003dhttp%3A%2F%2F10.148.128.39%3A8088%2Fproxy%2Fapplication_1539793487822_0001/stages/stage/?id\u003d674\u0026attempt\u003d0",
                "numCompleteTasks": 1200,
                "numActiveTasks": 0,
                "numFailedTasks": 0,
                "numTotalTasks": 1200
              },
              {
                "id": 671,
                "completed": false,
                "stageUrl": "https://api.qubole.com/cluster-proxy?encodedUrl\u003dhttp%3A%2F%2F10.148.128.39%3A8088%2Fproxy%2Fapplication_1539793487822_0001/stages/stage/?id\u003d671\u0026attempt\u003d0",
                "numCompleteTasks": 0,
                "numActiveTasks": 0,
                "numFailedTasks": 0,
                "numTotalTasks": 1020
              },
              {
                "id": 675,
                "completed": true,
                "stageUrl": "https://api.qubole.com/cluster-proxy?encodedUrl\u003dhttp%3A%2F%2F10.148.128.39%3A8088%2Fproxy%2Fapplication_1539793487822_0001/stages/stage/?id\u003d675\u0026attempt\u003d0",
                "numCompleteTasks": 1200,
                "numActiveTasks": 0,
                "numFailedTasks": 0,
                "numTotalTasks": 1200
              },
              {
                "id": 672,
                "completed": true,
                "stageUrl": "https://api.qubole.com/cluster-proxy?encodedUrl\u003dhttp%3A%2F%2F10.148.128.39%3A8088%2Fproxy%2Fapplication_1539793487822_0001/stages/stage/?id\u003d672\u0026attempt\u003d0",
                "numCompleteTasks": 1200,
                "numActiveTasks": 0,
                "numFailedTasks": 0,
                "numTotalTasks": 1200
              }
            ],
            "status": "Success"
          }
        ],
        "numCompletedTasks": 4801,
        "numTasks": 6841,
        "truncated": false
      },
      "version": "v1",
      "jobName": "paragraph_1539721293004_-1720985951",
      "id": "20181016-202133_55827573_q_KS1P94R8591539268868",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "+---+--------------------+\n| id|      scaledFeatures|\n+---+--------------------+\n|  1|[-0.4228786433090...|\n|  2|[-0.4228786433090...|\n|  3|[-0.4228786433090...|\n|  4|[-0.4228786433090...|\n|  5|[-0.4228786433090...|\n|  6|[-0.4228786433090...|\n|  7|[-0.4228786433090...|\n|  8|[-0.4228786433090...|\n|  9|[-0.4228786433090...|\n| 10|[1.36664608072298...|\n| 11|[-0.4228786433090...|\n| 12|[-0.4228786433090...|\n| 13|[-0.4228786433090...|\n| 14|[1.36664608072298...|\n| 15|[-0.4228786433090...|\n| 16|[-0.4228786433090...|\n| 17|[-0.4228786433090...|\n| 18|[-0.4228786433090...|\n| 19|[-0.4228786433090...|\n| 20|[1.36664608072298...|\n+---+--------------------+\nonly showing top 20 rows\n\n"
      },
      "dateCreated": "Oct 16, 2018 8:21:33 PM",
      "dateSubmitted": "Oct 17, 2018 9:18:03 PM",
      "dateStarted": "Oct 17, 2018 9:18:03 PM",
      "dateFinished": "Oct 17, 2018 9:18:15 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 1000
    },
    {
      "text": "",
      "config": {},
      "settings": {
        "params": {},
        "forms": {}
      },
      "version": "v0",
      "jobName": "paragraph_1539719710853_842556052",
      "id": "20181016-195510_1678171710_q_KS1P94R8591539268868",
      "dateCreated": "Oct 16, 2018 7:55:10 PM",
      "status": "READY",
      "progressUpdateIntervalMs": 1000
    },
    {
      "text": "def scale_sentiment(table, start_date, end_date, partition_column, pageviews_threshold \u003d 5, bot_threshold_percentage \u003d 0.0025 ):\n  \"\"\"\n  Returns the scaled column vector data for the selected date range \n  Note:\n     First the dataframe is pivoted into a dense matrix \n  \"\"\"\n  \n  from pyspark.ml.feature import StandardScaler\n  from pyspark.ml.feature import VectorAssembler\n  import numpy as np\n  from pyspark.ml.feature import IDF\n  \n  medium_high_cohorts_cleaned \u003d clean_sentiment(table, start_date, end_date,partition_column, pageviews_threshold, bot_threshold_percentage)  \n  \n  number_unique_sentiments \u003d medium_high_cohorts_cleaned.select(\u0027tag\u0027).distinct().count()\n  \n  # Pivot dataframe into a dense matrix\n  medium_high_cohorts_cleaned.groupBy([\"gup_anon_id\"]).pivot(\"tag\").sum(\"freq\").createOrReplaceTempView(\u0027medium_high_cohorts_pivoted_table\u0027)\n  \n  \n  # Add unique row numbers to be used as ID since the gup_anon_id is too long it cannot be used as ID since it gets truncated\n  \n  add_ids_sql \u003d \"\"\"\n        select *, row_number() over (order by gup_anon_id) as id, \u0027{partition_column}\u0027 as partition_column  from medium_high_cohorts_pivoted_table\n  \"\"\".format(partition_column \u003d partition_column)\n  \n  spark.sql(add_ids_sql).createOrReplaceTempView(\u0027medium_high_cohorts_pivoted_table_with_ids\u0027) \n\n  \n  # Write ID-gup_anon_id mapping\n  spark.sql(\"use lens_targeting\")\n\n  insert_into_table_sql \u003d \"\"\"insert into sentiment_mapping partition (partition_column) \n                             select gup_anon_id, id, partition_column from medium_high_cohorts_pivoted_table_with_ids\"\"\"\n  \n  spark.sql(insert_into_table_sql)\n  return spark.sql(\"select * from medium_high_cohorts_pivoted_table_with_ids limit 5 \")",
      "user": "fberhane@gannett.com",
      "dateUpdated": "Oct 17, 2018 8:53:05 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "version": "v1",
      "jobName": "paragraph_1539660652597_-1205758444",
      "id": "20181016-033052_1713320816_q_KS1P94R8591539268868",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": ""
      },
      "dateCreated": "Oct 16, 2018 3:30:52 AM",
      "dateSubmitted": "Oct 17, 2018 8:53:05 PM",
      "dateStarted": "Oct 17, 2018 8:53:05 PM",
      "dateFinished": "Oct 17, 2018 8:53:05 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 1000
    },
    {
      "text": "cost \u003d {}\nfor k in range(2, 100):\n  PCs_dataframe \u003d calculate_sentiment_pcs(start_date, end_date, pageviews_threshold, bot_threshold_percentage, variance_to_retain)\n  kmeans \u003d KMeans(maxIter\u003d50, featuresCol\u003d\"features\", predictionCol\u003d\"prediction\").setK(45).setSeed(1)\n  model \u003d kmeans.fit(PCs_dataframe)\n  transformed \u003d model.transform(PCs_dataframe)\n  transformed.select([\u0027id\u0027,\u0027prediction\u0027]).withColumn(\u0027table_appendix\u0027, f.lit(table_appendix)).createOrReplaceTempView(\u0027predictions_table\u0027)\n  \n  \n# Trains a k-means model.                           \n\n   kmeans \u003d KMeans(maxIter\u003d50, featuresCol\u003d\"features\", predictionCol\u003d\"prediction\").setK(k).setSeed(1)\n   model \u003d kmeans.fit(rescaledData)\n   cost[k] \u003d model.computeCost(rescaledData)\n   \n#print(cost) \nprint(\u0027Time elaspsed\u0027, timeit.default_timer() - start_time)\n\n\nmodel \u003d kmeans.fit(PCs_dataframe)",
      "user": "fberhane@gannett.com",
      "dateUpdated": "Oct 16, 2018 4:57:29 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "version": "v0",
      "jobName": "paragraph_1539708957311_-665133836",
      "id": "20181016-165557_1410855263_q_KS1P94R8591539268868",
      "dateCreated": "Oct 16, 2018 4:55:57 PM",
      "status": "READY",
      "progressUpdateIntervalMs": 1000
    },
    {
      "text": "",
      "config": {},
      "settings": {
        "params": {},
        "forms": {}
      },
      "version": "v0",
      "jobName": "paragraph_1539809326752_-1418235273",
      "id": "20181017-204846_423510572_q_KS1P94R8591539268868",
      "dateCreated": "Oct 17, 2018 8:48:46 PM",
      "status": "READY",
      "progressUpdateIntervalMs": 1000
    }
  ],
  "name": "Sentiment Clustering",
  "id": "KS1P94R8591539268868",
  "angularObjects": {
    "2DJH48GA9656181532106861473:shared_process": [],
    "2DKNJ64RE656181532106861488:shared_process": [],
    "2DM35ANMM656181532106861469:shared_process": [],
    "2DM2QAA2B656181532106861417:shared_process": []
  },
  "config": {
    "isDashboard": false,
    "defaultLang": ""
  },
  "info": {},
  "source": "FCN"
}